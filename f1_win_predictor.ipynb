{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7064effe",
   "metadata": {},
   "source": [
    "# Formula 1 Win Predictor\n",
    "This notebook will guide you through building a Formula 1 win predictor using various data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed080e3c",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498fc2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastf1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Configure fastf1\n",
    "cache_path = os.path.expanduser('~/.fastf1_cache')\n",
    "if not os.path.exists(cache_path):\n",
    "    os.makedirs(cache_path)\n",
    "fastf1.Cache.enable_cache(cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e1a5ef",
   "metadata": {},
   "source": [
    "## 2. Fetch and Aggregate Data from fastf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa494096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get the current year's schedule\n",
    "year = datetime.now().year\n",
    "schedule = fastf1.get_event_schedule(year)\n",
    "\n",
    "# Filter for events that have already happened\n",
    "schedule = schedule[schedule['EventDate'] < datetime.now()] \n",
    "\n",
    "# Load all race sessions\n",
    "races = []\n",
    "for i, row in schedule.iterrows():\n",
    "    if row['EventFormat'] == 'conventional' and row['Session5'] == 'Race':\n",
    "        try:\n",
    "            session = fastf1.get_session(year, row['RoundNumber'], 'R')\n",
    "            session.load()\n",
    "            races.append(session)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load session for {row['EventName']}: {e}\")\n",
    "\n",
    "# Combine lap data from all races\n",
    "if races:\n",
    "    all_laps = []\n",
    "    for race in races:\n",
    "        laps = race.laps\n",
    "        laps['RaceName'] = race.event['EventName']\n",
    "        laps['RaceDate'] = race.event['EventDate']\n",
    "        all_laps.append(laps)\n",
    "    \n",
    "    laps_df = pd.concat(all_laps, ignore_index=True)\n",
    "    print(\"Combined lap data from all races:\")\n",
    "    print(laps_df.head())\n",
    "    print(f\"Total laps loaded: {len(laps_df)}\")\n",
    "else:\n",
    "    print(\"No races found for the current year yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d3b02c",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9887f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "if 'laps_df' in locals():\n",
    "    # Convert LapTime to seconds\n",
    "    laps_df['LapTimeSeconds'] = laps_df['LapTime'].dt.total_seconds()\n",
    "\n",
    "    # Handle missing values - for now, just show where they are\n",
    "    print(\"Missing values in laps_df:\")\n",
    "    print(laps_df.isnull().sum())\n",
    "\n",
    "    # We see NaNs in LapTime, LapTimeSeconds, and some telemetry columns. This is expected for in/out laps.\n",
    "    # We will keep these for now as they can be useful for identifying pit stops.\n",
    "    \n",
    "    print(\"\\nCleaned DataFrame head:\")\n",
    "    print(laps_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1f032b",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2404064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "if 'laps_df' in locals() and races:\n",
    "    # --- Qualifying Data ---\n",
    "    all_quali_results = []\n",
    "    for race in races:\n",
    "        try:\n",
    "            # Ensure we only process conventional races for this logic\n",
    "            if race.event['EventFormat'] == 'conventional':\n",
    "                race.load_laps(with_telemetry=False) # ensure laps are loaded\n",
    "                quali = race.get_session('Q')\n",
    "                quali.load(laps=False, telemetry=False, weather=False, messages=False)\n",
    "                \n",
    "                results = quali.results\n",
    "                results['RaceName'] = race.event['EventName']\n",
    "                all_quali_results.append(results)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load qualifying data for {race.event['EventName']}: {e}\")\n",
    "\n",
    "    if all_quali_results:\n",
    "        quali_df = pd.concat(all_quali_results, ignore_index=True)\n",
    "        # Merge qualifying position into the laps dataframe\n",
    "        laps_df = pd.merge(laps_df, quali_df[['RaceName', 'Abbreviation', 'Position']], \n",
    "                           left_on=['RaceName', 'Driver'], \n",
    "                           right_on=['RaceName', 'Abbreviation'], \n",
    "                           how='left')\n",
    "        laps_df.rename(columns={'Position_y': 'QualiPosition', 'Position_x': 'Position'}, inplace=True)\n",
    "        laps_df.drop(columns=['Abbreviation'], inplace=True)\n",
    "        \n",
    "        print(\"Merged qualifying data:\")\n",
    "        print(laps_df[['RaceName', 'Driver', 'QualiPosition', 'Position']].head())\n",
    "\n",
    "    # --- Tyre Life and Pit Stops ---\n",
    "    laps_df['TyreLife'] = laps_df.groupby(['RaceName', 'Driver', 'Stint'])['LapNumber'].cumcount() + 1\n",
    "    laps_df['IsPitOutLap'] = laps_df['PitOutTime'].notna()\n",
    "\n",
    "    # --- Position Gained on Lap 1 ---\n",
    "    laps_df['PositionGainedLap1'] = np.nan # Initialize column\n",
    "    if 'QualiPosition' in laps_df.columns:\n",
    "        # Calculate for rows where QualiPosition is not NaN\n",
    "        mask = laps_df['LapNumber'] == 1\n",
    "        valid_quali = laps_df['QualiPosition'].notna()\n",
    "        lap1_df = laps_df[mask & valid_quali].copy()\n",
    "        \n",
    "        if not lap1_df.empty:\n",
    "            lap1_df['PositionGainedLap1'] = lap1_df['QualiPosition'] - lap1_df['Position']\n",
    "            \n",
    "            # Merge this back into the main dataframe\n",
    "            laps_df.set_index(['RaceName', 'Driver'], inplace=True)\n",
    "            laps_df.update(lap1_df.set_index(['RaceName', 'Driver'])['PositionGainedLap1'])\n",
    "            laps_df.reset_index(inplace=True)\n",
    "\n",
    "    print(\"\\nEngineered features:\")\n",
    "    print(laps_df[['Driver', 'LapNumber', 'TyreLife', 'IsPitOutLap', 'PositionGainedLap1']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5731b1d",
   "metadata": {},
   "source": [
    "## 5. Data Normalization and Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aff34ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "if 'laps_df' in locals() and not laps_df.empty:\n",
    "    # --- More Advanced Feature Engineering and Data Aggregation ---\n",
    "    # We want to predict the winner based on early race data.\n",
    "    # Let's use data from the first 10 laps.\n",
    "    early_laps_df = laps_df[laps_df['LapNumber'] <= 10].copy()\n",
    "\n",
    "    # Get final race result to define the target\n",
    "    # Find the final lap for each race\n",
    "    final_laps = laps_df.loc[laps_df.groupby('RaceName')['LapNumber'].idxmax()]\n",
    "    # Get the winner (Position 1) for each race from the final lap\n",
    "    race_winners = final_laps[final_laps['Position'] == 1][['RaceName', 'Driver']].copy()\n",
    "    \n",
    "    # Define aggregations\n",
    "    aggregations = {\n",
    "        'AvgLapTime': ('LapTimeSeconds', 'mean'),\n",
    "        'StdDevLapTime': ('LapTimeSeconds', 'std'),\n",
    "        'AvgPosition': ('Position', 'mean')\n",
    "    }\n",
    "\n",
    "    # Conditionally add aggregations for optional features\n",
    "    if 'QualiPosition' in early_laps_df.columns:\n",
    "        aggregations['QualiPosition'] = ('QualiPosition', 'first')\n",
    "    else:\n",
    "        print(\"Warning: 'QualiPosition' column not found in early_laps_df. Skipping aggregation for it.\")\n",
    "\n",
    "    if 'PositionGainedLap1' in early_laps_df.columns:\n",
    "        aggregations['PositionGainedLap1'] = ('PositionGainedLap1', 'first')\n",
    "    else:\n",
    "        print(\"Warning: 'PositionGainedLap1' column not found in early_laps_df. Skipping aggregation for it.\")\n",
    "\n",
    "    # Aggregate features per driver for each race\n",
    "    driver_features = early_laps_df.groupby(['RaceName', 'Driver']).agg(**aggregations).reset_index()\n",
    "\n",
    "    # If columns were skipped during aggregation, add them with NaNs for consistency\n",
    "    if 'QualiPosition' not in driver_features.columns:\n",
    "        driver_features['QualiPosition'] = np.nan\n",
    "    if 'PositionGainedLap1' not in driver_features.columns:\n",
    "        driver_features['PositionGainedLap1'] = np.nan\n",
    "\n",
    "\n",
    "    # Check if driver_features is empty before proceeding\n",
    "    if driver_features.empty:\n",
    "        print(\"No driver features could be aggregated. Skipping model training.\")\n",
    "    else:\n",
    "        # Merge with winner information to create the target variable\n",
    "        merged_df = pd.merge(driver_features, race_winners, on=['RaceName', 'Driver'], how='left', indicator=True)\n",
    "        merged_df['IsWinner'] = (merged_df['_merge'] == 'both').astype(int)\n",
    "        merged_df.drop('_merge', axis=1, inplace=True)\n",
    "\n",
    "        # Define features and target\n",
    "        features = merged_df.drop(columns=['RaceName', 'Driver', 'IsWinner'])\n",
    "        target = merged_df['IsWinner']\n",
    "\n",
    "        # Identify categorical and numerical features\n",
    "        categorical_features = features.select_dtypes(include=['object']).columns\n",
    "        numerical_features = features.select_dtypes(include=np.number).columns\n",
    "\n",
    "        # Create preprocessing pipelines for numerical and categorical features\n",
    "        numerical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())])\n",
    "        categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "        # Create a preprocessor object using ColumnTransformer\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numerical_transformer, numerical_features),\n",
    "                ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "        print(\"Preprocessor created for aggregated driver data.\")\n",
    "        print(f\"Number of features: {len(features.columns)}\")\n",
    "        print(f\"Features: {features.columns.tolist()}\")\n",
    "        print(f\"Number of races: {merged_df['RaceName'].nunique()}\")\n",
    "        print(f\"Number of winners: {target.sum()}\")\n",
    "else:\n",
    "    print(\"laps_df is not defined or is empty. Skipping feature engineering and model training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5d0606",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab592c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'features' in locals() and 'numerical_features' in locals():\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(features[numerical_features].corr(), annot=True, fmt=\".2f\")\n",
    "    plt.title('Correlation Heatmap of Numerical Features for Model Input')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Features for visualization are not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5b092f",
   "metadata": {},
   "source": [
    "## 7. Train/Test Split and Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21a3526",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'features' in locals() and 'target' in locals():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42, stratify=target)\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcc44bd",
   "metadata": {},
   "source": [
    "## 8. Model Selection and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bf5a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'preprocessor' in locals():\n",
    "    # Define models\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "        'Random Forest': RandomForestClassifier(random_state=42),\n",
    "        'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "    }\n",
    "\n",
    "    # Create and train pipelines\n",
    "    pipelines = {}\n",
    "    for name, model in models.items():\n",
    "        pipelines[name] = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                          ('classifier', model)])\n",
    "        print(f\"Training {name}...\")\n",
    "        pipelines[name].fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\nModels trained successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca1dd47",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7399fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'pipelines' in locals():\n",
    "    results = {}\n",
    "    for name, pipeline in pipelines.items():\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        results[name] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1-score': f1}\n",
    "        print(f\"--- {name} ---\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1-score: {f1:.4f}\")\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix - {name}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.show()\n",
    "\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    print(\"\\n--- Model Comparison ---\")\n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aebc68",
   "metadata": {},
   "source": [
    "## 10. Predicting Race Winners with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b7b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'pipelines' in locals() and 'X_test' in locals():\n",
    "    # Example: Predict on the test set\n",
    "    best_model_name = results_df['Accuracy'].idxmax()\n",
    "    best_model = pipelines[best_model_name]\n",
    "\n",
    "    predictions = best_model.predict(X_test)\n",
    "    prediction_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Create a DataFrame with the original test data, including identifiers\n",
    "    prediction_df = merged_df.loc[X_test.index].copy()\n",
    "    prediction_df['Actual_Winner'] = y_test\n",
    "    prediction_df['Predicted_Winner'] = predictions\n",
    "    prediction_df['Prediction_Probability'] = prediction_proba\n",
    "\n",
    "    print(f\"--- Predictions using {best_model_name} ---\")\n",
    "    # Display results for one race to make it easier to see the winner prediction\n",
    "    if not prediction_df.empty:\n",
    "        example_race = prediction_df['RaceName'].unique()[0]\n",
    "        print(f\"\\n--- Predictions for {example_race} ---\")\n",
    "        print(prediction_df[prediction_df['RaceName'] == example_race][['Driver', 'Actual_Winner', 'Predicted_Winner', 'Prediction_Probability']].sort_values(by='Prediction_Probability', ascending=False))\n",
    "    else:\n",
    "        print(\"No predictions to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bba7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Training and Evaluation with Cross-Validation ---\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Check if 'features' and 'target' are available\n",
    "if 'features' in locals() and 'target' in locals() and not features.empty:\n",
    "    # Create a pipeline with the preprocessor and the model\n",
    "    # Use class_weight='balanced' to handle imbalanced classes\n",
    "    model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                     ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced'))])\n",
    "\n",
    "    # Define the cross-validation strategy\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Define the scoring metrics\n",
    "    scoring = {\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'precision': make_scorer(precision_score, zero_division=0),\n",
    "        'recall': make_scorer(recall_score),\n",
    "        'f1_score': make_scorer(f1_score)\n",
    "    }\n",
    "\n",
    "    # Perform cross-validation\n",
    "    cv_results = cross_validate(model_pipeline, features, target, cv=cv, scoring=scoring)\n",
    "\n",
    "    # Print the cross-validation results\n",
    "    print(\"Cross-Validation Results:\")\n",
    "    print(f\"  Accuracy: {cv_results['test_accuracy'].mean():.4f} (+/- {cv_results['test_accuracy'].std():.4f})\")\n",
    "    print(f\"  Precision: {cv_results['test_precision'].mean():.4f} (+/- {cv_results['test_precision'].std():.4f})\")\n",
    "    print(f\"  Recall: {cv_results['test_recall'].mean():.4f} (+/- {cv_results['test_recall'].std():.4f})\")\n",
    "    print(f\"  F1-score: {cv_results['test_f1_score'].mean():.4f} (+/- {cv_results['test_f1_score'].std():.4f})\")\n",
    "\n",
    "    # Now, train the final model on the entire dataset for prediction\n",
    "    print(\"\\nTraining final model on the entire dataset...\")\n",
    "    model_pipeline.fit(features, target)\n",
    "    print(\"Final model trained.\")\n",
    "\n",
    "    # Store the trained model and the original dataframe for predictions\n",
    "    final_model = model_pipeline\n",
    "    prediction_df = merged_df.copy()\n",
    "\n",
    "else:\n",
    "    print(\"Features or target not available. Skipping model training.\")\n",
    "\n",
    "# --- Make Predictions and Interpret Results ---\n",
    "\n",
    "# Check if the final model and prediction data are available\n",
    "if 'final_model' in locals() and 'prediction_df' in locals() and not prediction_df.empty:\n",
    "    # Extract features for prediction\n",
    "    prediction_features = prediction_df.drop(columns=['RaceName', 'Driver', 'IsWinner'])\n",
    "\n",
    "    # Get predicted probabilities\n",
    "    predicted_probabilities = final_model.predict_proba(prediction_features)[:, 1]\n",
    "    prediction_df['PredictedWinnerProbability'] = predicted_probabilities\n",
    "\n",
    "    # For each race, find the driver with the highest predicted probability\n",
    "    predicted_winners = prediction_df.loc[prediction_df.groupby('RaceName')['PredictedWinnerProbability'].idxmax()]\n",
    "\n",
    "    # Get the actual winners for comparison\n",
    "    actual_winners = prediction_df[prediction_df['IsWinner'] == 1]\n",
    "\n",
    "    # Create a summary dataframe\n",
    "    results_summary = pd.merge(\n",
    "        predicted_winners[['RaceName', 'Driver', 'PredictedWinnerProbability']],\n",
    "        actual_winners[['RaceName', 'Driver']],\n",
    "        on='RaceName',\n",
    "        suffixes=['_Predicted', '_Actual']\n",
    "    )\n",
    "    results_summary.rename(columns={'Driver_Predicted': 'Predicted_Winner', 'Driver_Actual': 'Actual_Winner'}, inplace=True)\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    correct_predictions = (results_summary['Predicted_Winner'] == results_summary['Actual_Winner']).sum()\n",
    "    total_predictions = len(results_summary)\n",
    "    prediction_accuracy = (correct_predictions / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "\n",
    "    # Display the results\n",
    "    print(f\"Overall Prediction Accuracy: {prediction_accuracy:.2f}% ({correct_predictions}/{total_predictions})\\n\")\n",
    "    print(\"Prediction Summary:\")\n",
    "    display(results_summary[['RaceName', 'Predicted_Winner', 'Actual_Winner', 'PredictedWinnerProbability']])\n",
    "\n",
    "else:\n",
    "    print(\"Final model or prediction data not available. Skipping predictions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
